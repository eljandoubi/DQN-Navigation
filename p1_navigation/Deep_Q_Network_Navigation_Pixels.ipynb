{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Pixels\n",
    "---\n",
    "This notebook is based on the notebook of DQN coding exercice of udacity nanodegree deep reinforcement learning.\n",
    "\n",
    "We will train an agent to navigate in banana environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from agents import AgentQ\n",
    "from PIL import Image\n",
    "import torch, os, numpy as np, torch.optim as optim, matplotlib.pyplot as plt, models as M\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/VisualBanana.app\"`\n",
    "- **Windows** (x86): `\"path/to/VisualBanana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/VisualBanana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/VisualBanana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/VisualBanana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `VisualBanana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"VisualBanana.app\")\n",
    "```\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"VisualBanana_Windows_x86_64/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# the state size\n",
    "state_size = env_info.visual_observations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi1(X):\n",
    "    \" Transform a frame into grey scale image (array) \"\n",
    "    temp=np.round_(np.squeeze(X)*255).astype(np.uint8)\n",
    "    temp=np.array(Image.fromarray(temp).convert('L'))/255\n",
    "    return np.expand_dims(temp, axis=(0, 1))                   # batch_size x in_channels x image_size : 1 x 1 x 84 x 84\n",
    "    \n",
    "\n",
    "def phi2(X):\n",
    "    X=X.transpose((0,3,1,2))\n",
    "    return np.hstack((np.mean(X[:,:-1],axis=1,keepdims=True),X[:,-1:])) # an image with 2 channels yellow (red+green) and blue\n",
    "\n",
    "phi3=lambda im:im.transpose((0,3,1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "n_frames=4                    # Number of frame per state\n",
    "phi=phi3                      # set frame transformer \n",
    "in_channel=3*n_frames        # total number of channels\n",
    "\n",
    "buffer_size = 3*int(1e4)  # replay buffer size\n",
    "batch_size = 64         # minibatch size\n",
    "\n",
    "# initialize networks\n",
    "qnetwork_local = M.DConvQN(in_channel,action_size, seed)\n",
    "qnetwork_target = M.DConvQN(in_channel,action_size, seed)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam             # get an opimizer\n",
    "lr = 0.00005                       # learning rate \n",
    "param_opt = dict(lr=lr)            # the optimizer parameters\n",
    "device = torch.device(\"cuda\")      # set device\n",
    "\n",
    "\n",
    "agent = AgentQ(qnetwork_local, qnetwork_target, optimizer, param_opt, \n",
    "               action_size, seed, device, \"double\", buffer_size, batch_size, a=1., b=1.)               # define the agent \n",
    "\n",
    "\n",
    "check=\"Navigation_Pixels_checkpoint\"        # the checkpoint folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_Pixels(agent,path,n_frames,phi,n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent: the agent to train\n",
    "        path: checkpoint path\n",
    "        n_frames (int): number of frame per state\n",
    "        phi (callable): function transform the frame\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]                                  # reset the environment\n",
    "        Xs = deque([phi(np.zeros(state_size)),]*(n_frames-1),maxlen=n_frames)              # list of in_channel last frames\n",
    "        x = phi(env_info.visual_observations[0])                                           # get the current frame\n",
    "        Xs.append(x)                                                                       # save the new frame\n",
    "        state=np.hstack(Xs)                                                                # make state\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action= agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_x = phi(env_info.visual_observations[0])  # get the next frame\n",
    "            Xs.append(next_x)                              # save the new frame\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            next_state=np.hstack(Xs)                       # make state\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state=next_state\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        mean=np.mean(scores_window)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean))\n",
    "            save_path=os.path.join(path,'checkpoint_'+str(mean)+'.pth')\n",
    "            torch.save(agent.qnetwork_local.state_dict(),save_path)\n",
    "        \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = trainer_Pixels(agent,check,n_frames,phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch the Smart Agent!\n",
    "\n",
    "In the next code cell, we will load the trained weights and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "## get all checkpoints  \n",
    "checkpoints=[f for f in os.listdir(check) if f.startswith('checkpoint_') and f.endswith('.pth') ]\n",
    "\n",
    "## find the index of maximum score\n",
    "index=np.argmax([float(f[11:-4]) for f in checkpoints])\n",
    "\n",
    "## take the optimal checkpoint\n",
    "name=checkpoints[index]\n",
    "\n",
    "print(name)\n",
    "\n",
    "path=os.path.join(check,name)\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load(path))\n",
    "\n",
    "for i in range(10):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]      # reset the environment\n",
    "    Xs = deque([phi(np.zeros(state_size)),]*(n_frames-1),maxlen=n_frames)              # list of in_channel last frames\n",
    "    x = phi(env_info.visual_observations[0])                                           # get the current frame\n",
    "    Xs.append(x)                                                                       # save the new frame\n",
    "    state=np.hstack(Xs)                                                                # make state\n",
    "    score=0                                                 # initialise score\n",
    "    for j in range(1000):\n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_x = phi(env_info.visual_observations[0])  # get the next frame\n",
    "        Xs.append(next_x)                              # save the new frame\n",
    "        next_state=np.hstack(Xs)                       # make state\n",
    "        score += env_info.rewards[0]                   # update score\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        if done:\n",
    "            print(\"trail :\",i+1,\"score :\",score)\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
